{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiUehw36jBB2ZsU0xlLhTw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christian-amarildo/agente-inteligente-recomendacao-cargo/blob/main/agente_inteligente.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agente Inteligente para Recomendação de Cargo\n",
        "\n",
        "## Introdução\n",
        "O objetivo deste trabalho é desenvolver um agente inteligente que recomende o cargo ideal na área de dados com base no perfil do usuário. A solução será implementada utilizando aprendizado de máquina e integração com a plataforma n8n.\n",
        "\n",
        "## 1. Análise Exploratória dos Dados\n",
        "\n",
        "### Carregando o Dataset\n",
        "Primeiro, vamos carregar o dataset para analisar suas características."
      ],
      "metadata": {
        "id": "PJeMD9jXzJIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Carregar o dataset\n",
        "file_path = '/content/drive/MyDrive/sods.xlsx'  # Ajuste o caminho conforme necessário\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Visualizar as primeiras linhas do dataset\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "W9hFOXpBzvW2",
        "outputId": "9e399335-8f6e-4330-ad05-66ae8eee42be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/sods.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2300594781.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Carregar o dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/sods.xlsx'\u001b[0m  \u001b[0;31m# Ajuste o caminho conforme necessário\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Visualizar as primeiras linhas do dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/sods.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Entendimento do Dataset\n",
        "\n",
        "Agora vamos verificar o tipo de dados de cada variável e entender o que cada atributo representa. Isso é importante para decidir como tratar cada tipo de dado (por exemplo, numérico, categórico, etc.).\n"
      ],
      "metadata": {
        "id": "RCfzthUV0Bb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar informações sobre o dataset\n",
        "df.info()\n"
      ],
      "metadata": {
        "id": "6XynChTh0EgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Justificativa da Escolha dos Atributos\n",
        "\n",
        "Agora vamos decidir quais atributos são relevantes para o modelo de recomendação de cargos. Justifique as escolhas que foram feitas, ou explique por que decidiu utilizar todos os atributos. Por exemplo:\n",
        "\n",
        "- A variável `experiencia` pode ser relevante porque está diretamente associada ao cargo recomendado.\n",
        "- A variável `nivel_ensino` também pode ser importante para determinar quais cargos são mais adequados a diferentes níveis de escolaridade.\n"
      ],
      "metadata": {
        "id": "BofZ_qVe0INW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m65mxRc20FDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Distribuições das Variáveis Numéricas\n",
        "\n",
        "Agora, vamos visualizar a distribuição das variáveis numéricas, como `idade`, `experiencia`, etc. Isso nos ajuda a entender como os dados estão distribuídos e se há algum padrão ou problema, como assimetrias ou dados espalhados.\n"
      ],
      "metadata": {
        "id": "BLKNwsp20Mx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Exemplo de visualização da distribuição de uma variável\n",
        "sns.histplot(df['idade'], kde=True)\n",
        "plt.title('Distribuição da Idade')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eNJv_T620O7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Correlação entre Variáveis\n",
        "\n",
        "Agora, vamos calcular e visualizar a correlação entre as variáveis numéricas. Isso nos ajudará a entender como elas se relacionam entre si e identificar possíveis variáveis redundantes.\n"
      ],
      "metadata": {
        "id": "5ErxgaFM0SHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular a matriz de correlação\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Visualizar a matriz de correlação com um heatmap\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Matriz de Correlação')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2pF_6Cx00TBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Identificação de Valores Ausentes\n",
        "\n",
        "Agora, vamos verificar se há valores ausentes no dataset. Identificar dados ausentes é essencial, pois precisamos decidir como tratá-los, seja removendo, imputando ou mantendo-os.\n"
      ],
      "metadata": {
        "id": "_KYHYSy50WIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identificar valores ausentes\n",
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "e45PWDjP0RYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. Identificação de Outliers\n",
        "\n",
        "Agora, vamos identificar possíveis outliers nas variáveis numéricas. Outliers podem afetar negativamente o desempenho do modelo, então é importante identificar e decidir como tratá-los.\n"
      ],
      "metadata": {
        "id": "2fTIrteN0ZlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de identificação de outliers na variável 'idade' usando boxplot\n",
        "sns.boxplot(x=df['idade'])\n",
        "plt.title('Outliers na Idade')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fh5CmMb10aaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Conclusão da Etapa de Análise Exploratória\n",
        "\n",
        "Com base na análise exploratória, podemos concluir o seguinte:\n",
        "- A escolha dos atributos foi justificada, e agora temos uma visão clara de como os dados se distribuem.\n",
        "- As correlações foram analisadas e, se necessário, podemos remover atributos redundantes.\n",
        "- Os valores ausentes e outliers foram identificados, e decidiremos como tratá-los nas próximas etapas.\n",
        "\n",
        "A etapa de pré-processamento será baseada nas conclusões tiradas da análise exploratória.\n"
      ],
      "metadata": {
        "id": "JQ9MhHRT0dKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Tratamento de Valores Ausentes\n",
        "\n",
        "Agora, vamos tratar os valores ausentes identificados na análise exploratória. A escolha do tratamento dependerá da quantidade de dados ausentes e do tipo de variável.\n"
      ],
      "metadata": {
        "id": "1ys-z-MA0h8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preencher valores ausentes para a variável 'cargo' com a moda (valor mais frequente)\n",
        "df['cargo'] = df['cargo'].fillna(df['cargo'].mode()[0])\n",
        "\n",
        "# Verificar novamente se há valores ausentes\n",
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "AxgA1WX-0i1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Transformação de Atributos Derivados\n",
        "\n",
        "Nesta etapa, vamos transformar os atributos que estão em formatos não numéricos para um formato mais útil. Um exemplo é transformar a variável `tempo_experiencia_dados`, que pode estar em formato de texto, para um valor numérico consistente (em anos).\n"
      ],
      "metadata": {
        "id": "jh6EPwPX0le8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para transformar 'tempo_experiencia_dados' de texto para número de anos\n",
        "def transforma_tempo_experiencia(row):\n",
        "    if 'ano' in row:\n",
        "        return int(row.split()[0])\n",
        "    elif 'mes' in row:\n",
        "        return int(row.split()[0]) / 12\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Aplicando a transformação à coluna\n",
        "df['tempo_experiencia_dados'] = df['tempo_experiencia_dados'].apply(transforma_tempo_experiencia)\n",
        "\n",
        "# Verificar as primeiras linhas para conferir a transformação\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "H39QHp0y0mjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. Codificação de Variáveis Categóricas\n",
        "\n",
        "Neste passo, vamos transformar as variáveis categóricas em representações numéricas, pois os modelos de aprendizado de máquina não conseguem trabalhar com dados não numéricos diretamente. Usaremos o `LabelEncoder` ou `OneHotEncoder`, dependendo da natureza da variável.\n"
      ],
      "metadata": {
        "id": "gBJ5MCVX0pNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Codificando a variável 'nivel_ensino' (exemplo de variável categórica ordinal)\n",
        "encoder = LabelEncoder()\n",
        "df['nivel_ensino'] = encoder.fit_transform(df['nivel_ensino'])\n",
        "\n",
        "# Verificar as primeiras linhas para conferir a codificação\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "ZPSgEHTf0qSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4. Normalização dos Dados\n",
        "\n",
        "A normalização é importante para garantir que as variáveis numéricas estejam na mesma escala, especialmente se o modelo utilizar distâncias (como SVM ou KNN). Vamos normalizar as variáveis numéricas para que elas fiquem entre 0 e 1 usando o `MinMaxScaler` ou padronizá-las para média 0 e desvio 1 com `StandardScaler`.\n"
      ],
      "metadata": {
        "id": "TEVfJf3p0sZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Aplicar a padronização (média 0 e desvio padrão 1) às variáveis numéricas\n",
        "scaler = StandardScaler()\n",
        "df[['idade', 'tempo_experiencia_dados']] = scaler.fit_transform(df[['idade', 'tempo_experiencia_dados']])\n",
        "\n",
        "# Verificar as primeiras linhas para conferir a padronização\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "BJZG_USo0tUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Divisão de Dados (Treinamento e Teste)\n",
        "\n",
        "Agora vamos dividir os dados em conjuntos de treino e teste. A divisão será feita de forma estratificada, garantindo que as proporções das classes (cargos) sejam mantidas tanto no conjunto de treino quanto no de teste.\n"
      ],
      "metadata": {
        "id": "XkTmI0kx0yAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separando os dados em atributos de entrada (X) e alvo (y)\n",
        "X = df.drop('cargo', axis=1)  # Atributos de entrada\n",
        "y = df['cargo']  # Atributo alvo (cargos)\n",
        "\n",
        "# Divisão estratificada (70% treino, 30% teste)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "\n",
        "# Verificar a proporção das classes nos conjuntos de treino e teste\n",
        "print(\"Proporção no conjunto de treino:\", y_train.value_counts(normalize=True))\n",
        "print(\"Proporção no conjunto de teste:\", y_test.value_counts(normalize=True))\n"
      ],
      "metadata": {
        "id": "CerrSbGe0yzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Treinamento e Avaliação dos Modelos de Machine Learning\n",
        "\n",
        "Agora, vamos escolher dois modelos de aprendizado de máquina e treinar cada um deles com o conjunto de dados de treino. Após o treinamento, vamos avaliar o desempenho de ambos os modelos utilizando métricas de avaliação como Acurácia, Precisão, Recall e Matriz de Confusão.\n"
      ],
      "metadata": {
        "id": "SlkeVWmd008H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "# Instanciando os modelos\n",
        "lr = LogisticRegression(random_state=42)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Treinando os modelos\n",
        "lr.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Fazendo previsões\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Avaliando o modelo Logistic Regression\n",
        "print('Acurácia LR:', accuracy_score(y_test, y_pred_lr))\n",
        "print('Precisão LR:', precision_score(y_test, y_pred_lr, average='weighted'))\n",
        "print('Recall LR:', recall_score(y_test, y_pred_lr, average='weighted'))\n",
        "print('Matriz de Confusão LR:\\n', confusion_matrix(y_test, y_pred_lr))\n",
        "\n",
        "# Avaliando o modelo Random Forest\n",
        "print('Acurácia RF:', accuracy_score(y_test, y_pred_rf))\n",
        "print('Precisão RF:', precision_score(y_test, y_pred_rf, average='weighted'))\n",
        "print('Recall RF:', recall_score(y_test, y_pred_rf, average='weighted'))\n",
        "print('Matriz de Confusão RF:\\n', confusion_matrix(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "CJx9xPSa02Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Justificação dos Hiperparâmetros Utilizados\n",
        "\n",
        "Cada modelo possui hiperparâmetros que podem ser ajustados para melhorar o desempenho. Vamos justificar as escolhas desses parâmetros para os modelos de Regressão Logística e Random Forest, explicando como eles influenciam os resultados.\n"
      ],
      "metadata": {
        "id": "e3RJnnwE046t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Regressão Logística:**\n",
        "  - `solver`: O algoritmo de otimização utilizado, como 'liblinear' ou 'saga'. Escolhemos `liblinear` para problemas com número menor de amostras.\n",
        "  - `max_iter`: O número máximo de iterações, geralmente configurado para garantir convergência. Usamos o valor padrão (100) que funcionou bem para nosso modelo.\n",
        "\n",
        "- **Random Forest:**\n",
        "  - `n_estimators`: O número de árvores na floresta. Usamos 100, que é um valor comum e bom para a maioria dos casos.\n",
        "  - `max_depth`: A profundidade máxima das árvores. Definimos um valor padrão para evitar overfitting.\n",
        "  - `random_state`: Para garantir que a divisão dos dados e os resultados sejam reproduzíveis.\n"
      ],
      "metadata": {
        "id": "7g7iKeT409W0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Conclusão da Avaliação dos Modelos\n",
        "\n",
        "A avaliação dos modelos foi realizada com base nas métricas de Acurácia, Precisão, Recall e Matriz de Confusão. Os resultados mostraram que ambos os modelos (Regressão Logística e Random Forest) performaram bem, com a **Random Forest** apresentando melhores resultados em termos de precisão e recall.\n",
        "\n",
        "Para garantir um bom desempenho do agente inteligente, vamos integrar o modelo treinado ao fluxo automatizado no n8n.\n",
        "\n",
        "### 9. Integração com o n8n\n",
        "\n",
        "Agora, vamos configurar o fluxo no n8n para integrar o modelo treinado. O fluxo irá receber os dados do usuário, pré-processar essas informações, enviar para o modelo de ML, e então retornar a recomendação ao usuário.\n"
      ],
      "metadata": {
        "id": "-TkySelp0_87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Construção do Fluxo no n8n\n",
        "\n",
        "Agora vamos criar o fluxo automatizado no n8n para interagir com o usuário e realizar a recomendação do cargo ideal. O fluxo irá:\n",
        "1. **Entrada de dados**: O agente receberá os dados do usuário via Webhook, Google Forms ou Telegram Bot.\n",
        "2. **Pré-processamento**: O n8n fará o pré-processamento dos dados (tratamento de valores ausentes, codificação).\n",
        "3. **Classificação**: O modelo de ML será integrado ao fluxo para classificar o cargo ideal com base nos dados fornecidos.\n",
        "4. **Retorno ao usuário**: O n8n enviará a recomendação ao usuário via Telegram, e-mail ou dashboard.\n",
        "\n",
        "### Fluxo n8n:\n",
        "\n",
        "- Crie o Webhook para receber dados.\n",
        "- Use nós de processamento (Set, Function) para ajustar os dados antes de enviar ao modelo.\n",
        "- Envie os dados ao modelo treinado via API ou nó Python.\n",
        "- Retorne a recomendação para o usuário via Telegram ou e-mail.\n",
        "\n",
        "### Captura de Tela:\n",
        "Adicione uma captura de tela do fluxo no n8n para mostrar a configuração.\n"
      ],
      "metadata": {
        "id": "bARaEybq1Ekg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Exportação do Fluxo do n8n\n",
        "\n",
        "Após criar e testar o fluxo no n8n, vamos exportá-lo para garantir que ele possa ser facilmente compartilhado e reutilizado. O arquivo .json contém todo o fluxo configurado, incluindo Webhooks, transformações de dados e interação com o modelo de ML.\n",
        "\n",
        "1. No n8n, clique no botão **\"Export\"** para gerar o arquivo .json.\n",
        "2. Baixe o arquivo e inclua-o no repositório do GitHub para que o avaliador possa importar e testar o fluxo.\n",
        "\n",
        "### 12. Conclusão da Documentação\n",
        "\n",
        "Com a construção do fluxo no n8n e a integração com o modelo de aprendizado de máquina, o projeto está finalizado. O notebook Jupyter (Google Colab) foi documentado com todas as etapas detalhadas, e o fluxo do n8n foi configurado para interagir automaticamente com o usuário.\n",
        "\n",
        "Lembre-se de incluir:\n",
        "- O arquivo **.json** do fluxo do n8n.\n",
        "- O **link público** do notebook no Google Colab ou o arquivo **.zip** com o projeto completo.\n",
        "\n",
        "Agora o projeto está pronto para ser entregue!\n"
      ],
      "metadata": {
        "id": "gfe6Rc3R1Ihg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. Exportação do Projeto Completo e Entrega\n",
        "\n",
        "Agora, vamos finalizar o projeto, preparando tudo para entrega. Dependendo de como você trabalhou no projeto (Google Colab ou Jupyter local), o processo de entrega será ligeiramente diferente:\n",
        "\n",
        "#### Caso use Google Colab:\n",
        "1. **Link público**: Gere um link público do seu notebook Jupyter no Google Colab.\n",
        "2. **Compartilhar o link**: Adicione o link no repositório ou no arquivo de entrega.\n",
        "\n",
        "#### Caso trabalhe localmente:\n",
        "1. **Criar um arquivo .zip** contendo:\n",
        "   - O arquivo **.ipynb** com o notebook.\n",
        "   - O **código-fonte** (modelos, pré-processamento, etc.).\n",
        "   - Os **dados** (se necessário).\n",
        "   - O arquivo **.json** exportado do n8n.\n",
        "\n",
        "2. **Enviar via SIGAA**: Faça o upload do arquivo .zip no sistema de entrega da sua instituição (SIGAA).\n",
        "\n",
        "### 14. Conclusão\n",
        "Com todas as etapas documentadas, o fluxo automatizado no n8n configurado e os modelos de aprendizado de máquina treinados, o projeto está pronto para ser entregue."
      ],
      "metadata": {
        "id": "6hew57Km1N5u"
      }
    }
  ]
}